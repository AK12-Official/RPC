> 会随着版本的不断迭代而更新，以便和实际编码保持一致
>
> 最后校对：2026-01-23（以当前 `TestServer` / `TestClient` + Netty + Zookeeper + 负载均衡 + 重试白名单 实现为准）

## 0. 当前默认配置（重要）

- 注册中心：Zookeeper（Curator Client）
    - 连接串：`1.92.112.182:2181`
    - namespace（根）：`MyRPC`
- RPC 通信：Netty（当前“一次请求一次连接”）
- 重试机制：Guava Retry（仅对白名单服务开启）
    - 触发：任意异常 或 `RpcResponse.code == 500`
    - 等待：固定 2s
    - 次数：最多 3 次尝试
- 自定义协议头：`[2B messageType][2B serializerType][4B length][N bytes body]`
    - messageType：REQUEST=0 / RESPONSE=1
    - serializerType：Object=0 / JSON=1 / ProtoBuf(protostuff)=2
- 当前两端默认序列化：`JsonSerializer`（serializerType=1）
- 负载均衡：默认一致性哈希（`ConsistencyHashBalance`）
- 白名单（允许重试的服务）：ZK 节点 `/CanRetry/{serviceName}` + 客户端本地缓存 + 监听增量更新

## 1. 模块/类职责速览

- 服务端
    - `TestServer`：启动入口（示例 host=127.0.0.1, port=19999）
    - `ServiceProvider`：本地服务容器（接口全限定名 -> 实现对象），并负责向注册中心注册
    - `ZKServiceRegister`：向 ZK 写入服务名节点（持久）与地址节点（临时）
    - `NettyRPCServer`：Netty Server 启动与端口监听
    - `NettyServerInitializer`：装配 pipeline（Encoder/Decoder/业务Handler）
    - `NettyRPCServerHandler`：反射调用本地服务实现，生成 `RpcResponse`

- 客户端
    - `TestClient`：客户端入口（当前实现偏“压测/性能测试”风格）
    - `ClientProxy`：JDK 动态代理，拦截接口调用并封装 `RpcRequest`
    - `NettyRpcClient`：负责 serviceDiscovery + 建连 + 发请求 + 阻塞拿响应
    - `ZKServiceCenter`：服务发现（优先读本地缓存，miss 再读 ZK）
    - `serviceCache`：本地缓存
        - 服务地址缓存（serviceName -> addressList）
        - 重试白名单缓存（serviceName Set）
    - `watchZK`：CuratorCache 监听节点创建/删除，驱动本地缓存更新
    - `guavaRetry`：重试器（仅对白名单服务启用）
    - `LoadBalance`：负载均衡接口（Random/Round/ConsistencyHash）
    - `NettyClientInitializer` / `NettyClientHandler`：客户端 pipeline 与响应回填

- 通用
    - `RpcRequest` / `RpcResponse`：请求/响应模型
    - `Encoder` / `Decoder`：自定义编解码器
    - `Serializer`：序列化 SPI（Object/JSON/ProtoBuf）

## 2. Stage 1：服务端启动流程（以当前 TestServer 为准）

1. 创建服务实现：实例化 `UserServiceImpl`
2. 创建 `ServiceProvider(host, port)`
     - 初始化本地映射表 `Map<String,Object>`（接口全限定名 → 实现对象）
     - 初始化注册中心客户端：`ZKServiceRegister`（连接 Zookeeper，namespace=`MyRPC`）
3. 注册服务：`serviceProvider.provideServiceInterface(userService, canRetry)`
     - 遍历实现类实现的每个接口 `clazz`
         - 写入本地映射：`interfaceProvider.put(clazz.getName(), service)`
         - 注册到 ZK：`ZKServiceRegister.register(clazz.getName(), host:port, canRetry)`
     - ZK 节点结构（在 namespace=`MyRPC` 下）：
         - 持久节点：`/<serviceName>`（serviceName 是接口全限定名，例如 `common.service.UserService`）
         - 临时节点：`/<serviceName>/<host:port>`（服务下线会自动删除）
         - 若 `canRetry=true`，额外临时节点：`/CanRetry/<serviceName>`（表示该服务允许客户端重试）
4. 启动 Netty 服务器：`NettyRPCServer.start(port)`
     - 创建 `bossGroup`/`workGroup`
     - 绑定 pipeline：`NettyServerInitializer(serviceProvider)`
         - `Encoder(new JsonSerializer())`（出站）
         - `Decoder()`（入站）
         - `NettyRPCServerHandler(serviceProvider)`（入站业务处理）
     - bind 端口开始监听（示例 19999）

## 3. Stage 2：客户端启动并调用（以当前 TestClient 为准）

当前 `TestClient` 更偏向“压测/性能统计”：默认循环调用 `getUserByUserId(1)` 10000 次，并做少量预热。

典型调用方式仍然是：

```java
ClientProxy clientProxy = new ClientProxy(); // 不写死 ip & port（通过 ZK 服务发现）
UserService proxy = clientProxy.getProxy(UserService.class);
User user = proxy.getUserByUserId(1);
System.out.println(user);
```

### 3.1 ClientProxy 初始化（当前实现）
- `new ClientProxy()` 内部：
    - 创建 `ZKServiceCenter()`：连接 ZK，初始化本地缓存，启动 `watchZK` 监听；并在冷启动时拉取一次 `/CanRetry` 以填充白名单缓存
    - 创建 `NettyRpcClient(serviceCenter)`：用于后续的 `sendRequest`

### 3.2 客户端调用入口（关键分支点）
- 每次调用代理方法进入 `ClientProxy.invoke()`：
    1. 组装 `RpcRequest`
    2. 调用 `serviceCenter.checkRetry(interfaceName)`
    3. 若在白名单：走 `guavaRetry.sendServiceWithRetry(request, rpcClient)`
    4. 否则：只调用一次 `rpcClient.sendRequest(request)`

## 4. 端到端 DataFlow（一次 RPC 调用的完整时序）

### 4.1 步骤 1：创建代理对象
- `ClientProxy` 使用 JDK 动态代理创建 `UserService` 的代理对象
- 每次调用代理方法都会进入 `ClientProxy.invoke()`

### 4.2 步骤 2：构建 RpcRequest（客户端）
```java
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
    //构建Request
    RpcRequest rpcRequest = RpcRequest.builder()
            .interfaceName(method.getDeclaringClass().getName())
            .methodName(method.getName())
            .params(args).paramsType(method.getParameterTypes())
            .build();
}
```
- 从方法反射信息构建 RpcRequest，包含：
    - interfaceName: 接口全限定名（如 common.service.UserService）
    - methodName: 方法名（如 getUserByUserId）
    - params: 参数数组（如 [1]）
    - paramsType: 参数类型数组（如 [Integer.class]）


### 4.3 步骤 3：服务发现（客户端）
- `NettyRpcClient` 内部持有 `ZKServiceCenter`
- `sendRequest(request)` 时根据 `request.getInterfaceName()` 到 Zookeeper 查询：
    - 优先从本地缓存 `serviceCache` 拿 `/<serviceName>` 的 address 列表
    - 缓存 miss 才会去 ZK 读取 `/<serviceName>` 下的子节点列表
    - 拿到 addressList 后，交给负载均衡器 `LoadBalance.balance(addressList)` 选择目标地址
        - `ZKServiceCenter` 默认使用一致性哈希：`ConsistencyHashBalance`

### 4.4 步骤 4：发送请求（客户端）
（与旧版本不同：不再使用 `LengthFieldPrepender/ObjectEncoder`，而是自定义协议头 + JSON 序列化）

- 建立 Netty 连接（connect 到步骤 3 发现的 host:port）
- 通过 `channel.writeAndFlush(rpcRequest)` 发送请求
- 客户端 pipeline（`NettyClientInitializer`）：
    1. `Decoder`（入站：ByteBuf → Java 对象）
    2. `Encoder(JsonSerializer)`（出站：Java 对象 → ByteBuf）
    3. `NettyClientHandler`（入站：拿到 `RpcResponse` 写入 channel attribute 并 close）

客户端自定义协议（Encoder 写入的顺序）：

```
[2B messageType][2B serializerType][4B length][N bytes body]
messageType: REQUEST=0 / RESPONSE=1
serializerType: ObjectSerializer=0 / JsonSerializer=1
```

其中 `body` 是 `JsonSerializer.serialize(obj)` 的结果。

### 4.5 步骤 5：服务端接收并解码
- 服务端 pipeline（`NettyServerInitializer`）：
    1. `Encoder(JsonSerializer)`（出站）
    2. `Decoder`（入站）
    3. `NettyRPCServerHandler(serviceProvider)`（入站业务处理）
- 入站请求到达后，`Decoder` 按协议头读取：
    - `messageType=0` → 反序列化为 `RpcRequest`
    - 通过 `serializerType` 选择序列化器（当前 pipeline 使用的是 `JsonSerializer`）
- `JsonSerializer` 在反序列化 `RpcRequest` 时，会根据 `paramsType` 对 `params` 做类型修正（例如把 `JSONObject` 转回真实参数类型），确保服务端反射调用参数匹配。

### 4.6 步骤 6：服务端处理请求
```java
protected void channelRead0(ChannelHandlerContext ctx, RpcRequest request) throws Exception {
    //接收request，读取并调用服务
    RpcResponse response = getResponse(request);
    ctx.writeAndFlush(response);
    ctx.close();
}
```
- 从 ServiceProvider 获取服务实现对象
- 通过反射调用方法
- 构建 RpcResponse（成功或失败）

### 4.7 步骤 7：服务端返回响应
- `ctx.writeAndFlush(response)` 触发出站
- `Encoder(JsonSerializer)` 将 `RpcResponse` 写成 ByteBuf（同一套协议头）
- 服务端当前实现会在写回响应后 `ctx.close()`（一次请求处理完就断开连接）

### 4.8 步骤 8：客户端接收响应
```java
protected void channelRead0(ChannelHandlerContext ctx, RpcResponse response) throws Exception {
    // 接收到response, 给channel设计别名，让sendRequest里读取response
    AttributeKey<RpcResponse> key = AttributeKey.valueOf("RPCResponse");
    ctx.channel().attr(key).set(response);
    ctx.channel().close();
}
```
- 客户端入站 ByteBuf 先经过 `Decoder`，反序列化得到 `RpcResponse`
- `JsonSerializer` 在反序列化 `RpcResponse` 时会尝试把 `data` 从 `JSONObject` 转回真实类型（依赖 `RpcResponse.dataType`）
- `NettyClientHandler` 将响应存入 Channel 的 AttributeKey：`RPCResponse`，然后关闭连接

### 4.9 步骤 9：客户端获取结果
- `NettyRpcClient.sendRequest()` 阻塞等待 `channel.closeFuture().sync()`
- 连接关闭后，从 `channel.attr(AttributeKey.valueOf("RPCResponse"))` 取回 `RpcResponse`
- `ClientProxy.invoke()` 返回 `response.getData()` 作为本次远程调用结果

## 5. 注册中心 + 本地缓存监听（watchZK）

### 5.1 注册（Server -> ZK）

- `ZKServiceRegister.register(serviceName, address)`
    - 若 `/<serviceName>` 不存在：创建持久节点
    - 创建临时节点 `/<serviceName>/<host:port>`（服务下线自动删除）
    - 若 `canRetry=true`：创建临时节点 `/CanRetry/<serviceName>`（服务下线自动删除）

### 5.2 发现（Client -> cache/ZK）

- `ZKServiceCenter.serviceDiscovery(serviceName)`
    - 先查 `serviceCache.getServcieFromCache(serviceName)`
    - 若为空：读 ZK `client.getChildren().forPath("/" + serviceName)`
    - 调用 `loadBalance.balance(serviceList)` 选择目标地址

### 5.3 监听（ZK -> Client cache）

- `watchZK.watchToUpdate(...)` 使用 CuratorCache 监听 namespace 下的节点变化
    - 实际监听路径为 `"/"`（namespace 内根路径），会覆盖所有服务节点
    - 事件处理：
        - NODE_CREATED：解析 path `/<serviceName>/<host:port>`，写入本地缓存
        - NODE_DELETED：解析 path，删除本地缓存中的该地址
        - 对 `/CanRetry/<serviceName>`：更新本地“重试白名单缓存”（add/remove）

注意：当前代码里 `NODE_CHANGED` 分支存在，但服务地址节点本身通常不做“变更”，更多是 create/delete。

## 6. 重试（超时重传）与白名单如何参与流程

### 6.1 白名单来源
- 服务端在注册服务时，通过 `canRetry` 决定是否写入 `/CanRetry/<serviceName>` 节点。
- 客户端：
    - 启动时会对 `/CanRetry` 做一次冷启动拉取，填充本地白名单缓存
    - 后续通过 `watchZK` 监听节点变化进行增量更新

### 6.2 重试触发规则（当前实现）
重试实现位于 `guavaRetry`：
- `retryIfException()`：任意异常重试
- `retryIfResult(response -> response.code == 500)`：服务端返回失败码重试
- 固定等待 2 秒，最多 3 次尝试

> 注意：当前代码层面没有显式“超时阈值”（例如 200ms/1s）；因此更准确叫“失败重试”。若需要严格超时，需要在 Netty 客户端侧增加读写超时或请求级 timeout。

## 7. 备注（当前实现的关键约束 / 一线注意点）

1. 当前是“一次请求一次连接”：client/server handler 都会在处理完响应/请求后关闭 channel。
2. 当前 `Decoder` 未做半包/粘包的完整保护（读取前未检查 `readableBytes`），在复杂网络环境可能需要补充帧解码/边界处理（例如引入 LengthFieldBasedFrameDecoder 或在 Decoder 内做最小可读校验）。
3. 客户端本地缓存已包含：服务地址缓存 + 白名单缓存，均为并发容器；监听线程与业务线程并发时更安全。
4. 一致性哈希 `ConsistencyHashBalance` 当前实现使用 static 结构且每次 balance 会 init；在长时间运行/多服务场景下可能导致虚拟节点累积，应考虑去重或按服务维度维护哈希环。
5. 重试会放大“每次请求一次连接”的成本；若追求性能，优先考虑连接复用（长连接/连接池）。